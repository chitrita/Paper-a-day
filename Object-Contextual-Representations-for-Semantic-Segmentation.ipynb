{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style = \"color:rgb(50,120,229)\">Object-Contextual Representations for Semantic Segmentation</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style = \"color:rgb(50,120,229)\">Paper Details</font>\n",
    "\n",
    "1. **Authors**: Yuhui Yuan, Xilin Chen, Jingdong Wang\n",
    "2. **Paper Link**: https://arxiv.org/pdf/1909.11065v2.pdf\n",
    "3. **Category**: Semantic Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style = \"color:rgb(50,120,229)\">Introduction</font>\n",
    "\n",
    "**Semantic Segmentation**: Assigning a label to each pixel in an image.\n",
    "\n",
    "**Approach**: Contextual Aggregation.\n",
    "\n",
    "**Motivation**: Class label assigned to one pixel is the category of the object that the pixel belongs to.\n",
    "\n",
    "### <font style = \"color:rgb(8,133,37)\">What does Context mean?</font>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "The context of one position typically refers to a set of positions, e.g., the surrounding pixels.\n",
    "</div>\n",
    "\n",
    "If we refer to another paper (**Context Based Object Categorization: A Critical Survey**), Contextual Features are used to represent the interaction of an object with its surroundings. It can be divided into the following 3 categories:\n",
    "1. **Semantic Context** - This focuses on object co-occurence and allows to correct label of one object without affecting the label of other objects. For example, a tree is more likely to co-occur with a plant than a whale.\n",
    "2. **Spatial Context** - This focuses on the position of objects. For example, a dog is more likely to be present above grass and below sky rater than above sky and below grass. \n",
    "3. **Scale Context** - This focuses on relative size of objects. For example, a car is relatively smaller than a truck and not the other way round.\n",
    "\n",
    "### <font style = \"color:rgb(8,133,37)\">Approach</font>\n",
    "The approach discussed in the paper consists of the following 3 steps:\n",
    "\n",
    "1. **Coarse Soft Segmentation** - This involves dividing the contextual pixels (surrounding pixels) into soft object regions. The word \"soft\" here means that our focus is NOT on carrying out accurate segmentation.\n",
    "2. **Object Region Representation** - We use the soft segmentation obtained from the above step and the pixel representation to represent each object region.\n",
    "3. **Object-Contextual Representation** (OCR) - We use the output from the above 2 steps along with Pixel-Region relation to obtain the augmented representations.\n",
    "\n",
    "<img src=\"images/paper1/image01.png\" alt=\"Pipeline of the approach\" title=\"The Pipeline of the approach\" />\n",
    "<center><b>Figure 1</b>: The pipeline of the approach discussed in the paper. <a href=\"https://arxiv.org/pdf/1909.11065v2.pdf\">Source</a></center>\n",
    "\n",
    "### <font style = \"color:rgb(8,133,37)\">Differences</font>\n",
    "\n",
    "**OCR vs Multi-Scale Context**\n",
    "\n",
    "1. OCR differentiates contextual pixels which belong to the same class to the contextual pixels which belong to different class.\n",
    "2. Multi-Scale Context approach only differentiates pixels present at different positions.\n",
    "\n",
    "**OCR vs other Relational Context schemes**\n",
    "\n",
    "The approach discussed in the paper considers not only the object region representations but also the pixel and pixel-region relations, unlike other approaches.\n",
    "\n",
    "It should also be mentioned here that the current approach is also a relational context approach.\n",
    "\n",
    "**OCR vs Coarse-to-fine Segmentation**\n",
    "\n",
    "While \"Coarse-to-fine Segmentation\" is also followed in the current approach, the difference is the way the coarse segmentation is used. The OCR approach uses the coarse segmentation to generate a contextual representation, whereas the other approaches use it directly as an extra representation.\n",
    "\n",
    "**OCR vs Region-wise Segmentation**\n",
    "\n",
    "The region-wise segmentation first groups the pixels into **super pixels** which are then assigned a label. OCR on the other hand, uses the grouped regions to learn a better labelling for the pixels, instead of directly using them for segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style = \"color:rgb(50,120,229)\">Approach</font>\n",
    "\n",
    "It's now time to go into the mathematical details of the approach.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Semantic Segmentation:</b> Given $K$ classes, assign each pixel $p_i$ of image $I$ a label $l_i$ (which is one of the $K$ <b>unique</b> classes).\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style = \"color:rgb(50,120,229)\">References</font>\n",
    "1. Object-Contextual Representations for Semantic Segmentation - https://arxiv.org/pdf/1909.11065v2.pdf\n",
    "2. Context Based Object Categorization: A Critical Survey - https://vision.cornell.edu/se3/wp-content/uploads/2014/09/context_review08_0.pdf\n",
    "3. Jupyter Markdown - https://www.ibm.com/support/knowledgecenter/en/SSGNPV_1.1.3/dsx/markd-jupyter.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
